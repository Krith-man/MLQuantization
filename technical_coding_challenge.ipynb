{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-03T10:22:15.801221Z",
     "start_time": "2023-08-03T10:22:15.757283Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch import nn\n",
    "import utils\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare MNIST dataset\n",
    "For our project, we utilized the MNIST dataset. MNIST dataset includes images with digits 0-9 and the following block is responsible to download, extract split and form the dataset to train and test dataloaders. The ratio between train and test dataloaders is (80%-20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-03T10:22:15.816353Z",
     "start_time": "2023-08-03T10:22:15.764116Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader, test_loader = utils.prepare_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained model\n",
    "For this example, a well known deep learning model architecure called Lenet5 was selected. I have already trained the initial model over 10 epochs and in the following code block I loaded it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-03T10:22:15.818003Z",
     "start_time": "2023-08-03T10:22:15.793149Z"
    }
   },
   "outputs": [],
   "source": [
    " # create a model instance\n",
    "model_fp32 = utils.LeNet5()\n",
    "# Check saved model with 32 bits exists, otherwise train it\n",
    "if not os.path.isfile(\"models/model_32.pth\"):\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    losses = utils.train(model_fp32, train_loader, num_epochs=10)\n",
    "    # Save weights\n",
    "    torch.save(model_fp32.state_dict(), \"models/model_32.pth\")\n",
    "else:\n",
    "    # Load model\n",
    "    model_fp32.load_state_dict(torch.load(\"models/model_32.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code block, we run the inference part of the initial Lenet5 model 10 times to extract the mean accuracy, mean inference time and mean model size. Those metrics will be compared later with the quantization techniques results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-03T10:22:22.624643Z",
     "start_time": "2023-08-03T10:22:15.804989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type: Original - Size (KB): 249.89\n",
      "Accuracy: 70.95% - Elapsed time (seconds): 0.68\n"
     ]
    }
   ],
   "source": [
    "num_experiments = 10\n",
    "results = []\n",
    "avg_model_size, avg_time_evaluation, avg_accuracy = utils.calc_model_metrics(model_fp32, test_loader, num_experiments)\n",
    "print('Model type: Original - Size (KB): {}'.format(avg_model_size))\n",
    "print('''Accuracy: {}% - Elapsed time (seconds): {}'''.format(avg_accuracy, avg_time_evaluation))\n",
    "results.append([avg_model_size, avg_time_evaluation, avg_accuracy])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment with Quantization Techniques\n",
    "One technique to minimize the size of a ML model is quantitation. In particular, quantization refers to techniques for doing both computations and memory accesses with lower precision data. Theoretically, this technique affects as well inference time and accuracy. In this project, I implemented three techniques for quantizing neural networks in PyTorch as part of quantization tooling in the torch.quantization name-space. Below, I introduce all 3 of them, running experiments on the initial Lenet5 model we previously loaded. The experiments were executed 10 times and the mean accuracy, mean inference time and mean quantization model size were extracted and plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-03T10:22:22.627901Z",
     "start_time": "2023-08-03T10:22:22.625671Z"
    }
   },
   "outputs": [],
   "source": [
    "# Start experiments with quantitation methods\n",
    "torch.backends.quantized.engine = 'qnnpack'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dynamic Quantitation\n",
    "This method involves not just converting the weights to int8 - as happens in all quantization variants - but also converting the activations to int8 on the fly, just before doing the computation (hence “dynamic”). The computations will thus be performed using efficient int8 matrix multiplication and convolution implementations, resulting in faster compute. However, the activations are read and written to memory in floating point format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-08-03T10:22:22.628171Z"
    }
   },
   "outputs": [],
   "source": [
    "  # Dynamic Quantitation\n",
    "dquant_avg_model_size, dquant_avg_time_evaluation, dquant_avg_accuracy = utils.calc_dynamic_quant_metrics(model_fp32, test_loader, num_experiments)\n",
    "print('Model type: Dynamic-Quantitation - Size (KB): {}'.format(dquant_avg_model_size))\n",
    "print('''Accuracy: {}% - Elapsed time (seconds): {}'''.format(dquant_avg_accuracy, dquant_avg_time_evaluation))\n",
    "results.append([dquant_avg_model_size, dquant_avg_time_evaluation, dquant_avg_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post-Training Static Quantization\n",
    "Except for converting networks to use both integer arithmetic and int8 memory accesses, this method focuses on an additional step of first feeding batches of data through the network and computing the resulting distributions of the different activations (specifically, this is done by inserting “observer” modules at different points that record these distributions). This information is used to determine how specifically the different activations should be quantized at inference time. Importantly, this additional step allows us to pass quantized values between operations instead of converting these values to floats - and then back to ints - between every operation, resulting in a significant speed-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Post-Training Static Quantization\n",
    "quantitation_method = 'x86'\n",
    "stat_quant_avg_model_size, stat_quant_avg_time_evaluation, stat_quant_avg_accuracy = utils.calc_post_training_static_quant_metrics(model_fp32, test_loader, quantitation_method, num_experiments)\n",
    "print('Model type: Static-Quant - Size (KB): {}'.format(stat_quant_avg_model_size))\n",
    "print('''Accuracy: {}% - Elapsed time (seconds): {}'''.format(stat_quant_avg_accuracy, stat_quant_avg_time_evaluation))\n",
    "results.append([stat_quant_avg_model_size, stat_quant_avg_time_evaluation, stat_quant_avg_accuracy])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantization-aware training (QAT)\n",
    "QAT is the last one quantization technique that typically results in highest accuracy of these three. With QAT, all weights and activations are “fake quantized” during both the forward and backward passes of training: that is, float values are rounded to mimic int8 values, but all computations are still done with floating point numbers. Thus, all the weight adjustments during training are made while “aware” of the fact that the model will ultimately be quantized; after quantizing, therefore, this method usually yields higher accuracy than the other two methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Quantization Aware Training\n",
    "quant_aware_avg_model_size, quant_aware_avg_time_evaluation, quant_aware_avg_accuracy = utils.calc_quant_aware_training_metrics(model_fp32, train_loader, test_loader, quantitation_method,\n",
    "                                                                                                                                num_experiments)\n",
    "print('Model type: Quant Aware Training (QAT) - Size (KB): {}'.format(quant_aware_avg_model_size))\n",
    "print('''Accuracy: {}% - Elapsed time (seconds): {}'''.format(quant_aware_avg_accuracy, quant_aware_avg_time_evaluation))\n",
    "results.append([quant_aware_avg_model_size, quant_aware_avg_time_evaluation, quant_aware_avg_accuracy])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Print all results from our experiments in a dataframe format."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results, columns=['model_size', 'inference_time', 'accuracy'], index=[\"Original\", \"DQuant\", \"PT Static Quant\", \"QAT\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "df = df[['model_size', 'accuracy']].copy()\n",
    "# Create matplotlib figure\n",
    "fig = plt.figure()\n",
    "# Create matplotlib axes\n",
    "ax = fig.add_subplot(111)\n",
    "# Create another axes that shares the same x-axis as ax.\n",
    "ax2 = ax.twinx()\n",
    "width = 0.2\n",
    "df.model_size.plot(kind='bar', color='blue', ax=ax, width=width, position=0)\n",
    "df.accuracy.plot(kind='bar', color='red', ax=ax2, width=width, position=1)\n",
    "# Create custom legend for 2 yaxis\n",
    "blue_patch = mpatches.Patch(color='blue', label='Model size')\n",
    "red_patch = mpatches.Patch(color='red', label='Accuracy')\n",
    "plt.legend(handles=[red_patch, blue_patch])\n",
    "for bars in ax.containers:\n",
    "    ax.bar_label(bars, labels=['   %.2f' % value for value in bars.datavalues], color='b', fontsize=10)\n",
    "for bars in ax2.containers:\n",
    "    ax2.bar_label(bars, labels=['%.2f   ' % value for value in bars.datavalues], color='r', fontsize=10)\n",
    "# Set y labels, axis limits, rotation x-axis, and figure title\n",
    "ax.set_ylabel('Model Size (KB)')\n",
    "ax.set_ylim(0, 300)\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_ylim(0, 100)\n",
    "ax.xaxis.set_tick_params(rotation=0)\n",
    "plt.title(\"Comparison Model Size - Accuracy between Quantization Methods\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see from the figure above, all quantization methods drop significantly their model size, up to 3.3 times smaller. In the meantime, we observe that the accuracy remains constant for all approaches, and in some cases it might slightly improve. Thanks to bibliography, this is still possible."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
